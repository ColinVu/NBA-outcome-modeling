{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: 2015-2016.pkl\n",
      "  -> Assigned to TRAIN\n",
      "Found file: 2014-2015.pkl\n",
      "  -> Assigned to TRAIN\n",
      "Found file: 2013-2014.pkl\n",
      "  -> Assigned to TRAIN\n",
      "Found file: 2017-2018.pkl\n",
      "  -> Assigned to TEST\n",
      "Found file: 2016-2017.pkl\n",
      "  -> Assigned to VALIDATION\n",
      "Final shapes:\n",
      "  Train: X=(3804, 3, 508), y=(3804,)\n",
      "  Valid: X=(1260, 3, 508), y=(1260,)\n",
      "  Test:  X=(1264, 3, 508),  y=(1264,)\n",
      "Tensors ready:\n",
      "  X_train_t=torch.Size([3804, 3, 508]), y_train_t=torch.Size([3804])\n",
      "  X_val_t=  torch.Size([1260, 3, 508]),   y_val_t=  torch.Size([1260])\n",
      "  X_test_t= torch.Size([1264, 3, 508]),  y_test_t= torch.Size([1264])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "##################################################\n",
    "# 1) Define data directory + which seasons go where\n",
    "##################################################\n",
    "data_dir = \"../data/\"\n",
    "train_seasons = [\"2013-2014.pkl\", \"2014-2015.pkl\", \"2015-2016.pkl\"]\n",
    "val_seasons   = [\"2016-2017.pkl\"]\n",
    "test_seasons  = [\"2017-2018.pkl\"]\n",
    "\n",
    "X_train_list, y_train_list = [], []\n",
    "X_val_list,   y_val_list   = [], []\n",
    "X_test_list,  y_test_list  = [], []\n",
    "\n",
    "def load_pkl(path):\n",
    "    \"\"\"Helper to load a .pkl file: returns (X, y).\"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f, encoding='latin1')\n",
    "\n",
    "##################################################\n",
    "# 2) Read each file, bucket into train/val/test\n",
    "##################################################\n",
    "for fname in os.listdir(data_dir):\n",
    "    if not fname.endswith(\".pkl\"):\n",
    "        continue\n",
    "    file_path = os.path.join(data_dir, fname)\n",
    "    print(f\"Found file: {fname}\")\n",
    "    X_data, y_data = load_pkl(file_path)\n",
    "    \n",
    "    if fname in train_seasons:\n",
    "        X_train_list.append(X_data)\n",
    "        y_train_list.append(y_data)\n",
    "        print(\"  -> Assigned to TRAIN\")\n",
    "    elif fname in val_seasons:\n",
    "        X_val_list.append(X_data)\n",
    "        y_val_list.append(y_data)\n",
    "        print(\"  -> Assigned to VALIDATION\")\n",
    "    elif fname in test_seasons:\n",
    "        X_test_list.append(X_data)\n",
    "        y_test_list.append(y_data)\n",
    "        print(\"  -> Assigned to TEST\")\n",
    "    else:\n",
    "        print(\"  -> Not in any known season list (skipping or handle separately).\")\n",
    "\n",
    "##################################################\n",
    "# 3) Concatenate each split\n",
    "##################################################\n",
    "X_train = np.concatenate(X_train_list, axis=0) if X_train_list else np.empty((0,))\n",
    "y_train = np.concatenate(y_train_list, axis=0) if y_train_list else np.empty((0,))\n",
    "X_valid = np.concatenate(X_val_list,   axis=0) if X_val_list   else np.empty((0,))\n",
    "y_valid = np.concatenate(y_val_list,   axis=0) if y_val_list   else np.empty((0,))\n",
    "X_test  = np.concatenate(X_test_list,  axis=0) if X_test_list  else np.empty((0,))\n",
    "y_test  = np.concatenate(y_test_list,  axis=0) if y_test_list  else np.empty((0,))\n",
    "\n",
    "print(f\"Final shapes:\")\n",
    "print(f\"  Train: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"  Valid: X={X_valid.shape}, y={y_valid.shape}\")\n",
    "print(f\"  Test:  X={X_test.shape},  y={y_test.shape}\")\n",
    "\n",
    "##################################################\n",
    "# 4) Normalize using train stats only\n",
    "#    Assuming X is [N, 3, 508]\n",
    "##################################################\n",
    "X_mean = np.mean(X_train, axis=(0,1), keepdims=True)\n",
    "X_std  = np.std(X_train,  axis=(0,1), keepdims=True) + 1e-8\n",
    "\n",
    "X_train_norm = (X_train - X_mean) / X_std\n",
    "X_valid_norm = (X_valid - X_mean) / X_std\n",
    "X_test_norm  = (X_test  - X_mean) / X_std\n",
    "\n",
    "# Store the original y mean/std before normalizing (for later un-scaling if desired)\n",
    "orig_y_mean = y_train.mean()\n",
    "orig_y_std  = y_train.std()\n",
    "\n",
    "y_train_norm = (y_train - orig_y_mean) / orig_y_std\n",
    "y_valid_norm = (y_valid - orig_y_mean) / orig_y_std\n",
    "y_test_norm  = (y_test  - orig_y_mean) / orig_y_std  # needed if you plan to evaluate MSE in normalized space\n",
    "\n",
    "##################################################\n",
    "# 5) Convert to PyTorch Tensors\n",
    "##################################################\n",
    "X_train_t = torch.tensor(X_train_norm, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train_norm, dtype=torch.float32)\n",
    "X_val_t   = torch.tensor(X_valid_norm, dtype=torch.float32)\n",
    "y_val_t   = torch.tensor(y_valid_norm, dtype=torch.float32)\n",
    "X_test_t  = torch.tensor(X_test_norm,  dtype=torch.float32)\n",
    "y_test_t  = torch.tensor(y_test_norm,  dtype=torch.float32)\n",
    "\n",
    "print(f\"Tensors ready:\")\n",
    "print(f\"  X_train_t={X_train_t.shape}, y_train_t={y_train_t.shape}\")\n",
    "print(f\"  X_val_t=  {X_val_t.shape},   y_val_t=  {y_val_t.shape}\")\n",
    "print(f\"  X_test_t= {X_test_t.shape},  y_test_t= {y_test_t.shape}\")\n",
    "\n",
    "# Now you can feed X_train_t and y_train_t into your existing model and training loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model matching paper's description\n",
    "class NBANeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NBANeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(1524, 500)\n",
    "        self.fc2 = nn.Linear(500, 100)\n",
    "        self.fc3 = nn.Linear(100, 20)\n",
    "        self.fc4 = nn.Linear(20, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape input: [batch_size, 3, 508] -> [batch_size, 1524]\n",
    "        batch_size = x.size(0)\n",
    "        x = x.reshape(batch_size, -1)  # Flatten the 3Ã—508 to 1524\n",
    "        \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "network = NBANeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.005, weight_decay=1e-3)\n",
    "loss = nn.MSELoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 39568.132812, Val Loss: 35502.121094\n",
      "Epoch 100, Train Loss: 1254.787720, Val Loss: 1958.070801\n",
      "Epoch 200, Train Loss: 1247.250122, Val Loss: 1959.159912\n",
      "Epoch 300, Train Loss: 1239.743530, Val Loss: 1960.108521\n",
      "Epoch 400, Train Loss: 1230.189331, Val Loss: 1960.470215\n",
      "Epoch 500, Train Loss: 1215.447998, Val Loss: 1959.286499\n",
      "Epoch 600, Train Loss: 1190.578003, Val Loss: 1958.309326\n",
      "Epoch 700, Train Loss: 1144.134644, Val Loss: 1957.639893\n",
      "Epoch 800, Train Loss: 1140.628174, Val Loss: 1996.751221\n",
      "Epoch 900, Train Loss: 1071.019653, Val Loss: 1978.993286\n",
      "Epoch 1000, Train Loss: 1116.199097, Val Loss: 1956.599243\n",
      "Epoch 1100, Train Loss: 1293.598633, Val Loss: 2113.998535\n",
      "Epoch 1200, Train Loss: 1050.458252, Val Loss: 2113.176270\n",
      "Epoch 1300, Train Loss: 1005.559265, Val Loss: 2057.912109\n",
      "Epoch 1400, Train Loss: 955.917908, Val Loss: 2099.799316\n",
      "Epoch 1500, Train Loss: 933.894714, Val Loss: 2054.260010\n",
      "Epoch 1600, Train Loss: 899.722351, Val Loss: 2119.898193\n",
      "Epoch 1700, Train Loss: 920.148682, Val Loss: 2117.312012\n",
      "Epoch 1800, Train Loss: 948.198853, Val Loss: 2145.912598\n",
      "Epoch 1900, Train Loss: 846.397278, Val Loss: 2201.981201\n",
      "Epoch 2000, Train Loss: 1024.555298, Val Loss: 2539.060547\n",
      "Epoch 2100, Train Loss: 853.461304, Val Loss: 2205.219971\n",
      "Epoch 2200, Train Loss: 839.086365, Val Loss: 2490.460449\n",
      "Epoch 2300, Train Loss: 983.600098, Val Loss: 2070.376953\n",
      "Epoch 2400, Train Loss: 763.722656, Val Loss: 2241.115479\n",
      "Epoch 2500, Train Loss: 3179.050537, Val Loss: 18144.386719\n",
      "Epoch 2600, Train Loss: 1234.494385, Val Loss: 1953.776611\n",
      "Epoch 2700, Train Loss: 1208.854736, Val Loss: 1954.087646\n",
      "Epoch 2800, Train Loss: 1154.656006, Val Loss: 1982.191528\n",
      "Epoch 2900, Train Loss: 1133.937866, Val Loss: 2018.664673\n",
      "Epoch 3000, Train Loss: 1043.837402, Val Loss: 2028.021973\n",
      "Epoch 3100, Train Loss: 949.166077, Val Loss: 2027.314697\n",
      "Epoch 3200, Train Loss: 998.781738, Val Loss: 2125.924316\n",
      "Epoch 3300, Train Loss: 1138.802368, Val Loss: 1945.103516\n",
      "Epoch 3400, Train Loss: 1087.696655, Val Loss: 1961.813843\n",
      "Epoch 3500, Train Loss: 1044.605225, Val Loss: 1985.693481\n",
      "Epoch 3600, Train Loss: 1019.294434, Val Loss: 2009.682495\n",
      "Epoch 3700, Train Loss: 993.155518, Val Loss: 2038.142822\n",
      "Epoch 3800, Train Loss: 868.058533, Val Loss: 2016.849243\n",
      "Epoch 3900, Train Loss: 926.994202, Val Loss: 1998.698242\n",
      "Epoch 4000, Train Loss: 1051.287598, Val Loss: 1967.444458\n",
      "Epoch 4100, Train Loss: 676.238586, Val Loss: 2001.375854\n",
      "Epoch 4200, Train Loss: 664.014465, Val Loss: 2004.491699\n",
      "Epoch 4300, Train Loss: 619.463989, Val Loss: 2009.395630\n",
      "Epoch 4400, Train Loss: 1029.749023, Val Loss: 2018.875366\n",
      "Epoch 4500, Train Loss: 953.356628, Val Loss: 2099.726807\n",
      "Epoch 4600, Train Loss: 3382.971924, Val Loss: 2774.413818\n",
      "Epoch 4700, Train Loss: 1154.660767, Val Loss: 1949.842041\n",
      "Epoch 4800, Train Loss: 1128.931885, Val Loss: 1963.865479\n",
      "Epoch 4900, Train Loss: 1106.984497, Val Loss: 1976.912476\n",
      "Epoch 5000, Train Loss: 1088.349731, Val Loss: 1993.899414\n",
      "Epoch 5100, Train Loss: 1073.566040, Val Loss: 2015.460327\n",
      "Epoch 5200, Train Loss: 1061.314087, Val Loss: 2037.136108\n",
      "Epoch 5300, Train Loss: 1049.117554, Val Loss: 2059.649170\n",
      "Epoch 5400, Train Loss: 1039.563110, Val Loss: 2084.883057\n",
      "Epoch 5500, Train Loss: 1025.232422, Val Loss: 2124.618164\n",
      "Epoch 5600, Train Loss: 1001.707275, Val Loss: 2141.249268\n",
      "Epoch 5700, Train Loss: 984.465698, Val Loss: 2173.011230\n",
      "Epoch 5800, Train Loss: 968.421814, Val Loss: 2193.471924\n",
      "Epoch 5900, Train Loss: 957.761047, Val Loss: 2232.649170\n",
      "Epoch 6000, Train Loss: 983.204163, Val Loss: 2095.533447\n",
      "Epoch 6100, Train Loss: 958.289185, Val Loss: 2167.542969\n",
      "Epoch 6200, Train Loss: 943.235413, Val Loss: 2185.973145\n",
      "Epoch 6300, Train Loss: 968.004089, Val Loss: 2268.156982\n",
      "Epoch 6400, Train Loss: 940.940674, Val Loss: 2257.556396\n",
      "Epoch 6500, Train Loss: 916.130005, Val Loss: 2226.051025\n",
      "Epoch 6600, Train Loss: 912.491943, Val Loss: 2253.300781\n",
      "Epoch 6700, Train Loss: 945.508484, Val Loss: 2367.393311\n",
      "Epoch 6800, Train Loss: 873.741333, Val Loss: 2257.397949\n",
      "Epoch 6900, Train Loss: 905.905518, Val Loss: 2346.840576\n",
      "Epoch 7000, Train Loss: 865.294312, Val Loss: 2311.371582\n",
      "Epoch 7100, Train Loss: 856.580688, Val Loss: 2368.914795\n",
      "Epoch 7200, Train Loss: 2316.402100, Val Loss: 2806.195068\n",
      "Epoch 7300, Train Loss: 1068.458496, Val Loss: 2073.199707\n",
      "Epoch 7400, Train Loss: 1009.418884, Val Loss: 2102.745850\n",
      "Epoch 7500, Train Loss: 987.440613, Val Loss: 2134.564209\n",
      "Epoch 7600, Train Loss: 981.853577, Val Loss: 2156.254883\n",
      "Epoch 7700, Train Loss: 974.628601, Val Loss: 2177.220703\n",
      "Epoch 7800, Train Loss: 914.144409, Val Loss: 2134.966309\n",
      "Epoch 7900, Train Loss: 1117.442383, Val Loss: 2389.392090\n",
      "Epoch 8000, Train Loss: 825.538452, Val Loss: 2124.514160\n",
      "Epoch 8100, Train Loss: 793.649170, Val Loss: 2108.099609\n",
      "Epoch 8200, Train Loss: 1042.245605, Val Loss: 2020.609741\n",
      "Epoch 8300, Train Loss: 998.384521, Val Loss: 2093.052002\n",
      "Epoch 8400, Train Loss: 985.012939, Val Loss: 2126.087402\n",
      "Epoch 8500, Train Loss: 1302.603271, Val Loss: 3513.522217\n",
      "Epoch 8600, Train Loss: 991.476074, Val Loss: 2053.625244\n",
      "Epoch 8700, Train Loss: 973.939819, Val Loss: 2109.472168\n",
      "Epoch 8800, Train Loss: 968.416687, Val Loss: 2136.780762\n",
      "Epoch 8900, Train Loss: 965.758484, Val Loss: 2158.276367\n",
      "Epoch 9000, Train Loss: 968.014343, Val Loss: 2154.351562\n",
      "Epoch 9100, Train Loss: 967.061951, Val Loss: 2160.652344\n",
      "Epoch 9200, Train Loss: 961.044434, Val Loss: 2157.007080\n",
      "Epoch 9300, Train Loss: 984.923035, Val Loss: 2185.080078\n",
      "Epoch 9400, Train Loss: 971.012146, Val Loss: 2164.290771\n",
      "Epoch 9500, Train Loss: 937.552979, Val Loss: 2153.495361\n",
      "Epoch 9600, Train Loss: 971.713379, Val Loss: 2188.419434\n",
      "Epoch 9700, Train Loss: 947.900146, Val Loss: 2192.093018\n",
      "Epoch 9800, Train Loss: 952.609741, Val Loss: 2181.551758\n",
      "Epoch 9900, Train Loss: 928.576660, Val Loss: 2180.537842\n"
     ]
    }
   ],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_valid, dtype=torch.float32)\n",
    "\n",
    "# Training loop with validation phase\n",
    "for t in range(10000):\n",
    "    # Training phase\n",
    "    network.train()                         # Set the model to training mode\n",
    "    y_train_pred = network(X_train)\n",
    "    train_loss = loss(y_train_pred.squeeze(), y_train)\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation phase\n",
    "    network.eval()                          # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        y_val_pred = network(X_valid)\n",
    "        val_loss = loss(y_val_pred.squeeze(), y_valid)\n",
    "\n",
    "    # Print every 100 epochs\n",
    "    if t % 100 == 0:\n",
    "        true_train_loss = train_loss.item()\n",
    "        true_val_loss   = val_loss.item()  \n",
    "        print(f\"Epoch {t}, Train Loss: {true_train_loss:.6f}, Val Loss: {true_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Complex Fully Connected / LSTM Neural Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedBasketballNN(nn.Module):\n",
    "    def __init__(self, input_size=508, hidden_size=256, lstm_layers=2, output_size=1):\n",
    "        super(AdvancedBasketballNN, self).__init__()\n",
    "        \n",
    "        # Bidirectional LSTM for capturing temporal dynamics in both directions\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.3 if lstm_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism to focus on important timesteps\n",
    "        self.attention = nn.Linear(hidden_size*2, 1)  # *2 for bidirectional\n",
    "        \n",
    "        # Deeper fully connected layers with batch normalization\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size*2)\n",
    "        self.fc1 = nn.Linear(hidden_size*2, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.fc4 = nn.Linear(128, output_size)\n",
    "        \n",
    "        # Advanced regularization\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        \n",
    "    def attention_mechanism(self, lstm_output):\n",
    "        # lstm_output shape: (batch_size, seq_len, hidden_size*2)\n",
    "        attention_weights = torch.softmax(self.attention(lstm_output), dim=1)\n",
    "        # Apply attention weights to LSTM output\n",
    "        context_vector = torch.sum(attention_weights * lstm_output, dim=1)\n",
    "        return context_vector\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len=3, features=508)\n",
    "        \n",
    "        # Process sequence with bidirectional LSTM\n",
    "        lstm_output, _ = self.lstm(x)\n",
    "        # lstm_output shape: (batch_size, seq_len, hidden_size*2)\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        context = self.attention_mechanism(lstm_output)\n",
    "        \n",
    "        # Apply batch normalization\n",
    "        context = self.bn1(context)\n",
    "        \n",
    "        # Deep fully connected layers with residual connections\n",
    "        residual = context\n",
    "        out = torch.relu(self.fc1(context))\n",
    "        out = self.bn2(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = torch.relu(self.fc2(out))\n",
    "        out = self.bn3(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = torch.relu(self.fc3(out))\n",
    "        out = self.bn4(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Final output layer\n",
    "        out = self.fc4(out)\n",
    "        \n",
    "        return out\n",
    "network = AdvancedBasketballNN(input_size=508, hidden_size=256, lstm_layers=2, output_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishkatariya/dev/CS Homework/4641/CS4641-Final-Project/.venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# More sophisticated optimizer with learning rate scheduling\n",
    "optimizer = torch.optim.AdamW(network.parameters(), lr=0.005, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 40330.464844, Val Loss: 43311.515625\n",
      "Epoch 26, Train Loss: 39290.156250, Val Loss: 40246.968750\n",
      "Epoch 51, Train Loss: 36117.578125, Val Loss: 36545.511719\n",
      "Epoch 76, Train Loss: 31135.705078, Val Loss: 33071.394531\n",
      "Epoch 101, Train Loss: 25103.230469, Val Loss: 26073.726562\n",
      "Epoch 126, Train Loss: 18501.537109, Val Loss: 19847.583984\n",
      "Epoch 151, Train Loss: 12156.049805, Val Loss: 13233.961914\n",
      "Epoch 176, Train Loss: 6597.234375, Val Loss: 7542.374023\n",
      "Epoch 201, Train Loss: 2817.803955, Val Loss: 3285.694580\n",
      "Epoch 226, Train Loss: 1551.450195, Val Loss: 1985.763550\n",
      "Epoch 251, Train Loss: 1513.460938, Val Loss: 1998.112305\n",
      "Epoch 276, Train Loss: 1510.474487, Val Loss: 1990.132568\n",
      "Epoch 301, Train Loss: 1501.399536, Val Loss: 1996.257690\n",
      "Epoch 326, Train Loss: 1489.084595, Val Loss: 1999.914307\n",
      "Epoch 351, Train Loss: 1499.808960, Val Loss: 2000.380615\n",
      "Epoch 376, Train Loss: 1502.905396, Val Loss: 2000.390503\n",
      "Epoch 401, Train Loss: 1510.060913, Val Loss: 2000.561523\n",
      "Epoch 426, Train Loss: 1505.166504, Val Loss: 2000.539673\n"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 20\n",
    "patience_counter = 0\n",
    "\n",
    "for t in range(1000):  # Increase epochs for better training\n",
    "    # Training phase\n",
    "    network.train()\n",
    "    y_pred = network(X_train)\n",
    "    train_loss = loss(y_pred.squeeze(), y_train)\n",
    "    \n",
    "    optimizer.zero_grad() \n",
    "    train_loss.backward()\n",
    "    # Gradient clipping to prevent exploding gradients\n",
    "    torch.nn.utils.clip_grad_norm_(network.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation phase\n",
    "    network.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = network(X_valid)\n",
    "        val_loss = loss(y_pred.squeeze(), y_valid)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(network.state_dict(), 'best_basketball_model.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        \n",
    "    # if patience_counter >= patience:\n",
    "    #     print(f\"Early stopping at epoch {t+1}\")\n",
    "    #     break\n",
    "        \n",
    "    if t % 25 == 0:  # Print every 25 epochs\n",
    "        print(f\"Epoch {t+1}, Train Loss: {train_loss.item():.6f}, Val Loss: {val_loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a proper model for sequential data\n",
    "class BasketballNN(nn.Module):\n",
    "    def __init__(self, input_size=508, hidden_size=100, output_size=1):\n",
    "        super(BasketballNN, self).__init__()\n",
    "        \n",
    "        # LSTM to handle sequential data (3 timesteps)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True  # expects input shape: (batch, seq, features)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers after processing the sequence\n",
    "        self.fc1 = nn.Linear(hidden_size, 50)\n",
    "        self.fc2 = nn.Linear(50, output_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len=3, features=508)\n",
    "        \n",
    "        # Process sequence with LSTM\n",
    "        # out shape: (batch_size, seq_len, hidden_size)\n",
    "        out, (hidden, _) = self.lstm(x)\n",
    "        \n",
    "        # Get the output from the last timestep\n",
    "        # hidden shape: (1, batch_size, hidden_size)\n",
    "        out = hidden.squeeze(0)\n",
    "        \n",
    "        # Process through fully connected layers\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Create the model with proper dimensions\n",
    "network = BasketballNN(input_size=508, hidden_size=100, output_size=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
